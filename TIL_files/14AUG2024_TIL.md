모두가 들어본적 있는 챗 GPT, 오늘은 LLM에 대해 최신 동향이나 근황에 대해 알아 보았다.  
최근에 들어본 모델로는 BARD, BERT, LLaMA 정도가 있다.  
대규모 언어 모델(LLM)은 방대한 양의 데이터로 사전 학습된 초대형 딥 러닝 모델이다. 기본 트랜스포머는 셀프 어텐션(self-attention) 기능을 갖춘 인코더와 디코더로 구성된 신경망 세트다. 인코더와 디코더는 일련의 텍스트에서 의미를 추출하고 텍스트 내의 단어와 구문 간의 관계를 이해한다.

트랜스포머 LLM은 비지도 학습이 가능하지만 더 정확한 설명은 트랜스포머가 자체 학습을 수행한다는 것이다. 이 과정을 통해 트랜스포머는 기본 문법, 언어 및 지식을 이해하는 법을 배운다.

입력을 순차적으로 처리하는 이전의 순환 신경망(RNN)과 달리 트랜스포머 전체 시퀀스를 병렬로 처리한다. 이를 통해 데이터 사이언티스트는 GPU를 사용하여 트랜스포머 기반 LLM을 학습할 수 있어 훈련 시간을 크게 줄일 수 있다.

트랜스포머 신경망 아키텍처를 사용하면 종종 수천억 개의 파라미터가 포함된 매우 큰 모델을 사용할 수 있다. 이러한 대규모 모델은 대개 인터넷뿐만 아니라 500억 개 이상의 웹 페이지로 구성된 Common Crawl과 약 5,700만 페이지로 구성된 Wikipedia와 같은 소스에서도 엄청난 양의 데이터를 수집할 수 있다고 한다.

참고 문헌:https://aws.amazon.com/ko/what-is/large-language-model/