```
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score, classification_report

class KNNClassifier:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X_train, y_train):
        """ Store the training data """
        self.X_train = X_train
        self.y_train = y_train

    def euclidean_distance(self, x1, x2):
        """ Compute the Euclidean distance between two points """
        return np.sqrt(np.sum((x1 - x2) ** 2))

    def predict(self, X_test):
        """ Predict the label for each test sample """
        predictions = [self._predict_single(x) for x in X_test]
        return np.array(predictions)

    def _predict_single(self, x):
        """ Predict a single test sample """
        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.y_train[i] for i in k_indices]
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]

# Load the Iris dataset
iris = load_iris()
X = iris.data[:, :2]  # We use only the first two features for easy visualization
y = iris.target

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the k-NN classifier with k=3
k = 3
knn = KNNClassifier(k=k)

# Train the classifier using the training data
knn.fit(X_train, y_train)

# Make predictions on the test data
y_pred = knn.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, y_pred))

# Visualization of decision boundaries and test predictions
def plot_decision_boundaries(X_train, y_train, k, X_test=None, y_test=None, y_pred=None):
    # Create a mesh grid for plotting decision boundaries
    h = 0.1
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    grid_points = np.c_[xx.ravel(), yy.ravel()]

    # Predict labels for the entire grid
    Z = knn.predict(grid_points)
    Z = Z.reshape(xx.shape)

    # Plot the decision boundaries
    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)

    # Plot the training data points
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, edgecolor='k', cmap=plt.cm.RdYlBu, label='Training Points')

    # Plot test data points if provided
    if X_test is not None and y_test is not None:
        plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=100, marker='x', cmap=plt.cm.RdYlBu, label='Test Points')

    # If predictions are provided, highlight correct and incorrect predictions
    if y_pred is not None:
        correct = y_test == y_pred
        plt.scatter(X_test[correct, 0], X_test[correct, 1], s=150, edgecolor='green', facecolor='none', label='Correct Predictions')
        plt.scatter(X_test[~correct, 0], X_test[~correct, 1], s=150, edgecolor='red', facecolor='none', label='Incorrect Predictions')

    # Labels and legend
    plt.xlabel(iris.feature_names[0])
    plt.ylabel(iris.feature_names[1])
    plt.title(f'k-NN Decision Boundaries (k={k})')
    plt.legend()
    plt.show()

# Plot the decision boundaries and the test data points
plot_decision_boundaries(X_train, y_train, k=k, X_test=X_test, y_test=y_test, y_pred=y_pred)

```

이 프로젝트는 Python과 인기 있는 라이브러리들인 numpy, matplotlib, sklearn을 사용하여 K-최근접 이웃(KNN) 분류기를 직접 구현한 코드입니다. 

코드의 구조와 기능에 대한 설명입니다.

개요
K-최근접 이웃(K-Nearest Neighbors) 알고리즘은 새로운 샘플을 학습 데이터의 특징 공간에서 가장 가까운 k개의 이웃과 비교하여 다수결로 분류하는 간단한 비모수적 방법입니다.  
이 구현에서는 유클리드 거리(Euclidean distance)를 이용하여 점들 사이의 거리를 계산합니다.

사용된 라이브러리  
numpy: 행렬 및 벡터 연산 등 수치 연산을 처리하기 위한 라이브러리입니다.  
matplotlib.pyplot: 시각화를 위한 라이브러리입니다.
collections.Counter: 가장 많이 등장한 클래스의 빈도를 계산하는 데 사용됩니다.  
sklearn:
1:train_test_split: 데이터셋을 학습용과 테스트용으로 나누기 위해 사용됩니다.  
2:load_iris: 분류 작업에 자주 사용되는 Iris 데이터셋을 불러옵니다.  
3:accuracy_score, classification_report: 모델 성능을 평가하는 데 사용됩니다.  

코드 설명
1. KNNClassifier 클래스  
초기화 (__init__)  
KNNClassifier 클래스는 k개의 이웃 수를 매개변수로 받아 초기화됩니다:  
```
class KNNClassifier:
    def __init__(self, k=3):
        self.k = k
```
기본적으로 k 값은 3으로 설정되어 있지만, 객체를 생성할 때 원하는 값으로 변경할 수 있습니다.

모델 학습 (fit)  
fit 메서드는 학습 데이터를 저장합니다:
```
def fit(self, X_train, y_train):
    self.X_train = X_train
    self.y_train = y_train
```

유클리드 거리 계산 (euclidean_distance)  
두 점 사이의 유클리드 거리는 다음과 같이 계산됩니다:
```
def euclidean_distance(self, x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))
```
이 함수는 각 테스트 샘플과 학습 샘플 간의 거리를 측정하는 데 사용됩니다.

여러 테스트 샘플 예측 (predict)  
predict 메서드는 테스트 샘플들을 반복하면서 각 샘플의 레이블을 _predict_single 메서드를 통해 예측합니다:
```
def predict(self, X_test):
    predictions = [self._predict_single(x) for x in X_test]
    return np.array(predictions)
```

  
개별 샘플 예측 (_predict_single)  
각 테스트 샘플에 대해 모든 학습 샘플과의 거리를 계산하고 이를 정렬합니다.   
k개의 최근접 이웃을 선택한 후, 가장 많이 등장한 클래스를 예측 결과로 반환합니다:
```
def _predict_single(self, x):
    distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]
    k_indices = np.argsort(distances)[:self.k]
    k_nearest_labels = [self.y_train[i] for i in k_indices]
    most_common = Counter(k_nearest_labels).most_common(1)
    return most_common[0][0]
```


2. 모델 평가
sklearn.datasets.load_iris를 이용하여, 이 분류기는 Iris 데이터셋으로 학습 및 테스트할 수 있습니다. accuracy_score와 classification_report 함수는 모델의 성능을 평가하는 데 사용됩니다.


학교에서 배운 내용을 정리해보았고 구현한 코드를 다루어보았습니다.